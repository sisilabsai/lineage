# Phase 2 Quick Reference: Training Loop Integration

**Status**: âœ“ COMPLETE  
**Timeline**: 3-5 days (Weeks 2-3)  
**Goal**: Integrate ML training with Arena and AgentMetrics

---

## ðŸŽ¯ What Was Accomplished

### âœ“ Fixed & Enhanced Core Training Components
- **optimizer.rs**: Fixed duplicate methods, corrected type signatures
- **rewards.rs**: Complete reward calculation system (5 components)
- **replay_buffer.rs**: Experience storage with capacity management
- **coordinator.rs**: Episode orchestration (now compiles)

### âœ“ Integration Points
- **Arena** â†’ Collect experiences from trading episodes
- **AgentMetrics** â†’ Read capital, drawdown, win_rate, scars
- **Neural Network** â†’ Forward passes for Q-value prediction
- **Training Loop** â†’ Bellman equation + SGD updates

### âœ“ Example & Documentation
- `examples/training_loop_example.rs` - Full working example
- `PHASE_2_TRAINING_COMPLETE.md` - Comprehensive guide
- Unit tests for all core modules

---

## ðŸ“Š Architecture Overview

```
Market Data
    â†“
[Arena Round] â† Executes trades with agent
    â†“
[Metric Update] â† AgentMetrics reflect results
    â†“
[Reward Calc] â† Convert metrics â†’ reward signal
    â†“
[Experience Store] â† (state, action, reward, next_state, done)
    â†“
[Replay Buffer] â† Sample batch for training
    â†“
[Q-Learning] â† Forward pass â†’ Bellman target â†’ Loss â†’ Backprop
    â†“
[Updated Weights] â† Agent ready for next episode
```

---

## ðŸ”§ Key Files

| File | Purpose | Status |
|------|---------|--------|
| `src/finance/ml/training/optimizer.rs` | Q-Learning trainer | âœ“ Fixed |
| `src/finance/ml/training/rewards.rs` | Reward calculation | âœ“ Working |
| `src/finance/ml/training/coordinator.rs` | Episode orchestration | âœ“ Working |
| `src/finance/ml/training/replay_buffer.rs` | Experience storage | âœ“ Working |
| `src/finance/ml/models/q_net.rs` | Neural network | âœ“ Working |
| `examples/training_loop_example.rs` | Usage example | âœ“ Compiles |

---

## ðŸ’¡ Usage Pattern

### Minimal Example
```rust
let model = SimpleQNet::new(5, 64)?;
let mut trainer = QLearningTrainer::new(model);

// Collect experience
trainer.remember_experience(
    vec![price1, price2, price3, price4, price5],
    action,           // 0-3
    reward,           // From RewardCalculator
    vec![...],        // Next state
    false,            // Not terminal
);

// Train on batch
let loss = trainer.train_step()?;
```

### Full Episode
```rust
let mut coordinator = TrainingCoordinator::new(model);

for episode in 0..100 {
    let metrics = coordinator.run_training_episode(
        &mut agent,
        &market_history,
        10_000,  // Initial capital
    )?;
    
    println!("Episode {}: Reward={:.2}", episode, metrics.reward);
}
```

---

## ðŸŽ“ Reward System

### Immediate Reward (per-trade)
```
Base reward = 0

if capital_increased:
    reward += (capital_gain / initial_capital) Ã— 10

if capital_decreased:
    reward -= (capital_loss / initial_capital) Ã— 20

if drawdown_increased:
    reward -= drawdown_increase Ã— 5

if new_scars:
    reward -= num_scars Ã— 1

if win_rate_improved:
    reward += (win_rate_improvement / 100) Ã— 2
```

### Episode Reward (terminal)
```
Base reward = 0

roi = (final_capital - initial_capital) / initial_capital
reward += roi Ã— 1000

reward -= max_drawdown Ã— 5
reward -= scar_count Ã— 10

if win_rate > 50:
    reward += (win_rate - 50) Ã— 2

if capital < initial_capital / 2:
    reward -= 1000  # Bankruptcy penalty
```

---

## ðŸ“ˆ Training Hyperparameters

```rust
gamma: 0.99              // Discount future rewards by 99%
learning_rate: 0.001     // Conservative SGD step
batch_size: 32           // Experiences per gradient update
replay_capacity: 10_000  // Max buffer size
episodes: N              // Number of training episodes
```

### Expected Performance

After 100 episodes:
- **Buffer Utilization**: 90%+ full
- **Average Loss**: 0.1 - 0.5 (MSE)
- **Episode Reward**: +500 to +2000
- **Win Rate**: 55-65% (from 50%)
- **Drawdown Reduction**: 20-30%

---

## ðŸ”¬ Testing

### Run Example
```bash
cargo run --example training_loop_example --features ml
```

### Run Tests
```bash
cargo test --lib finance::ml::training --features ml
```

### Expected Output
```
Episode 1: Trades: 18 | Reward: 245.32 | Loss: 0.234567
Episode 2: Trades: 20 | Reward: 512.15 | Loss: 0.195234
...
Training Statistics:
  Loss: 0.123456 | Steps: 100 | Buffer: 3200/10000
Replay Buffer Status:
  Capacity: 10000
  Current Size: 3200
  Utilization: 32.0%
```

---

## ðŸŽ¯ Bellman Equation (Core Algorithm)

For each experience (s, a, r, s', done):

```
Q(s,a) target = r + Î³ Ã— max(Q(s'))  if not done
               = r                    if done

Loss = (Q(s,a) - target)Â²

Update: Î¸ â† Î¸ - Î± Ã— âˆ‡Loss
```

**In English**:
1. Predict Q-values for current state
2. Predict Q-values for next state
3. Bellman target = immediate reward + discount Ã— best future
4. Training loss = difference between prediction and target
5. Gradient descent updates neural network weights

---

## ðŸ”— Integration Checklist

- [x] Rewards calculated from AgentMetrics
- [x] Experiences stored in replay buffer
- [x] Q-learning updates via Bellman equation
- [x] Training coordinates with Arena rounds
- [x] Example demonstrates full pipeline
- [x] Tests validate all components
- [x] Project compiles with `--features ml`
- [x] Performance metrics tracked

---

## âš™ï¸ Troubleshooting

### Loss not decreasing
â†’ Check reward scaling, increase batch size, verify data diversity

### Agent crashes (capital â†’ 0)
â†’ Increase drawdown penalty, add bankruptcy filter, reduce learning rate

### High variance in episodes
â†’ Increase batch size (64), lower learning rate (0.0001), normalize rewards

### Buffer not filling
â†’ Run more episodes, reduce capacity if memory-bound

---

## ðŸš€ What's Next (Phase 3)

| Feature | Priority | Timeline |
|---------|----------|----------|
| Target Network | High | Week 4 |
| Dueling DQN | High | Week 4 |
| Model Persistence | Medium | Week 5 |
| Prioritized Experience Replay | Medium | Week 5 |
| Multi-Agent Training | Low | Week 6+ |

---

## ðŸ“ Command Reference

```bash
# Build with ML features
cargo build --features ml

# Run example
cargo run --example training_loop_example --features ml

# Run tests
cargo test --lib finance::ml --features ml -- --nocapture

# Check documentation
cargo doc --open --features ml

# Full build
cargo build --release --features ml
```

---

## ðŸ“š File Structure

```
src/finance/ml/
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ mod.rs                 # Module exports
â”‚   â”œâ”€â”€ optimizer.rs           # Q-Learning trainer âœ“
â”‚   â”œâ”€â”€ coordinator.rs         # Episode orchestration âœ“
â”‚   â”œâ”€â”€ rewards.rs             # Reward calculation âœ“
â”‚   â””â”€â”€ replay_buffer.rs       # Experience storage âœ“
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ q_net.rs               # Neural network
â”‚   â””â”€â”€ ...
â””â”€â”€ ...

examples/
â””â”€â”€ training_loop_example.rs   # Full example âœ“
```

---

## ðŸ† Success Metrics

âœ“ **Compilation**: `cargo build --features ml` succeeds  
âœ“ **Example**: Runs without errors  
âœ“ **Tests**: All tests pass  
âœ“ **Integration**: Works with Arena and AgentMetrics  
âœ“ **Documentation**: Complete with examples  
âœ“ **Performance**: Loss decreases, rewards improve  

---

## ðŸ“ž Key Contacts/Resources

- **Neural Network**: `SimpleQNet` in `src/finance/ml/models/q_net.rs`
- **Agent Metrics**: `AgentMetrics` in `src/finance/agent.rs`
- **Arena Integration**: `src/finance/arena.rs` (training hooks)
- **Example**: `examples/training_loop_example.rs`

---

## ðŸŽ“ Learning Resources Embedded

### In Code
- Detailed comments in each module
- Docstrings on all public methods
- Example implementation in `training_loop_example.rs`

### In Documentation
- Bellman equation derivation
- Hyperparameter tuning guide
- Troubleshooting section
- Architecture diagrams

---

**Status**: âœ“ Phase 2 Training Integration COMPLETE

Ready for: Phase 3 (Advanced Features: Target Networks, Dueling DQN, etc.)

---

## Version History

- **v0.2.1** - Phase 2 Training Complete
- **v0.2.0** - Phase 1 Core ML Systems
- **v0.1.0** - Initial Lineage System
